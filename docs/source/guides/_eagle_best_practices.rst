.. _eagle-best-practices:

Best Practices
====================

This page collects practical recommendations for achieving the best results when training EAGLE
speculative decoding models.


.. _eagle-best-practices-data-synthesis:

Data Synthesis
--------------

Training on conversations **generated by the base model** rather than human-authored datasets
significantly improves token acceptance rates. The draft module learns to predict the target
model's actual output distribution, not just surface-level text patterns.

To prepare synthetic training data, launch an inference server with the base model:

.. code-block:: bash

    pip install vllm
    vllm serve meta-llama/Llama-3.2-1B-Instruct \
        --api-key token-abc123 \
        --port 8000 \
        --tensor-parallel-size 1

.. note::

    For quantized models, add ``--quantization=modelopt`` to the ``vllm serve`` command.

Then generate conversations using prompts from your training dataset:

.. code-block:: bash

    python scripts/server_generate.py \
        --data_path input_conversations/daring-anteater.jsonl \
        --output_path synthetic/train.jsonl

Use ``--system_prompt <text>`` to inject a system prompt into every conversation.

For large-scale generation across multiple GPUs, see the
`SLURM data preparation guide <https://github.com/NVIDIA/Model-Optimizer/blob/main/examples/speculative_decoding/SLURM_prepare_data.md>`_.


.. _eagle-best-practices-configure:

Configuring the Draft Model
----------------------------

ModelOpt ships with sensible default architectures for EAGLE‑1 and EAGLE‑3. See
:ref:`eagle-config-reference` for the full list of configurable fields.

When launching training via ``launch_train.sh``, pass a JSON override file with
``--eagle_config <file>``. Only the fields you want to change need to be specified; omitted
fields fall back to the built-in defaults. For example, to use a 2-layer draft decoder with a
larger MLP:

.. code-block:: json

    {
        "num_hidden_layers": 2,
        "intermediate_size": 8192
    }

See :ref:`eagle-config-reference` for more details.


.. _eagle-best-practices-vocab:

Draft Vocabulary Compression
-----------------------------

By default the draft model shares the full vocabulary of the base model. For large vocabularies
(e.g., 128 256 tokens in Llama‑3) you can compress the draft vocabulary to a smaller working set,
reducing embedding table size and speeding up both training and inference.

**Step 1 — Calibrate a vocabulary mapping**

Find the most frequently used tokens in your training set and save a ``d2t.pt`` mapping file:

.. code-block:: bash

    python scripts/calibrate_draft_vocab.py \
        --model meta-llama/Llama-3.2-1B-Instruct \
        --data input_conversations/daring-anteater.jsonl \
        --draft_vocab_size 32000 \
        --save_dir draft_vocab_cache

The ``d2t.pt`` file maps each compressed draft token index to its offset in the target vocabulary.
During inference the target token is recovered as:

.. code-block:: text

    target_token = draft_token_index + d2t[draft_token_index]

**Step 2 — Enable compressed vocabulary in training**

Add the following to your ``eagle_config.json``:

.. code-block:: json

    {"draft_vocab_size": 32000}

Then pass ``--draft_vocab_cache <path_to_d2t.pt>`` when running ``./launch_train.sh``. The draft
model will use the compressed vocabulary table during both training and export.
