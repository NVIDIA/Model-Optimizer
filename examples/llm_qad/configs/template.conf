#!/bin/bash
########################################################
# QAD Training Configuration Template
########################################################
# Copy this file and modify for your experiment:
#   cp template.conf my-experiment.conf
#
# Then run:
#   sbatch --nodes=4 -t 4:00:00 sbatch_qwen_qad.sh --config configs/my-experiment.conf
########################################################

########################################################
# MODEL CONFIGURATION
########################################################

# Student model architecture name
export STUDENT_MODEL="Qwen3-8B"

# Teacher model architecture name
export TEACHER_MODEL="Qwen3-8B"

# Model architecture config file (optional - auto-detected from STUDENT_MODEL)
# export STUDENT_CONFIG_FILE="/path/to/Megatron-LM/.../conf/Qwen/Qwen3-8B.sh"

########################################################
# CHECKPOINTS (REQUIRED)
########################################################

# Student checkpoint path (REQUIRED)
# This is the FP4 quantized checkpoint for QAD training
export STUDENT_CKPT="/path/to/student/checkpoint"

# Teacher checkpoint path (REQUIRED for QAD mode)
# This is the BF16 teacher model for knowledge distillation
export TEACHER_CKPT="/path/to/teacher/checkpoint"

# Teacher model config YAML (REQUIRED)
# Contains: num_layers, hidden_size, num_attention_heads, ffn_hidden_size
export TEACHER_MODEL_CONFIG="/path/to/teacher.yaml"

########################################################
# TRAINING CONFIGURATION
########################################################

# Learning rate
export LR="1e-6"

# Dataset name (selects from predefined datablends)
# Options: openscience, nemotron, nemotron_v2, combined, slimorca
export DATASET_NAME="openscience"

# Training samples (leave empty to use dataset default)
# export TRAIN_SAMPLES=""

# KD config file path (optional - for custom distillation settings)
# export KD_CFG_PATH="/path/to/kd_config.yaml"

########################################################
# PARALLELISM CONFIGURATION
########################################################

# Tensor Parallelism (must match checkpoint TP)
export TP_SIZE=8

# Pipeline Parallelism (increase for larger models)
export PP_SIZE=1

# Expert Parallelism (for MoE models)
export EP_SIZE=1

# Micro-batch size per GPU
export MBS=16

# Number of GPUs per node
export NUM_GPUS=8

# Master port for distributed training
export MASTER_PORT=29500

########################################################
# ROOT PATHS
########################################################

# Megatron-LM source directory
export MLM_DIR="/lustre/fsw/coreai_dlalgo_modelopt/weimingc/workspace/Megatron-LM"

# ModelOpt source directory
export MODELOPT_DIR="/lustre/fsw/coreai_dlalgo_modelopt/weimingc/workspace/TensorRT-Model-Optimizer"

# Root directory containing model checkpoints
export MODELS_ROOT="/lustre/fsw/coreai_dlalgo_modelopt/weimingc/models"

# Root directory for training outputs
export QAD_CHECKPOINT_ROOT="/lustre/fsw/coreai_dlalgo_modelopt/weimingc/checkpoints"

# Data cache directory
export DATACACHE_DIR="/lustre/fsw/coreai_dlalgo_modelopt/weimingc/data_cache"

########################################################
# DATASET PATHS (optional)
########################################################

# Custom datablend JSON path (overrides DATASET_NAME)
# export BLEND_PATH="/path/to/datablend.json"

########################################################
# CONTAINER CONFIGURATION
########################################################

export CONTAINER_IMAGE="/lustre/fsw/coreai_dlalgo_modelopt/weimingc/containers/pytorch_25.06-py3.sqsh"
export CONTAINER_MOUNTS="/lustre/fsw:/lustre/fsw"
export CONTAINER_WORKDIR="/lustre/fsw/coreai_dlalgo_modelopt/weimingc/workspace/TensorRT-Model-Optimizer/examples/llm_qad"

########################################################
# ADVANCED OPTIONS
########################################################

# HuggingFace token for accessing gated models (avoids rate limiting)
# Recommended: pass via --hf-token arg to avoid logging
# Example: sbatch sbatch_qwen_qad.sh --hf-token hf_xxx --config ...
# Or set via environment: export HF_TOKEN="hf_xxx"

# Iterations to skip (comma-separated)
# export ITERATIONS_TO_SKIP=""
