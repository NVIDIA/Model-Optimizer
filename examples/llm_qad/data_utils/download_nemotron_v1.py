#!/usr/bin/env python3
"""
Download and preprocess NVIDIA Nemotron-Post-Training-Dataset-v1 for QAD training.

This dataset contains high-quality reasoning data generated by DeepSeek-R1 and Qwen3-235B,
which is excellent for improving MMLU and reasoning capabilities.

Splits available:
- stem: 20.6M samples (science, reasoning, humanities) - BEST for MMLU
- math: 2.0M samples (step-by-step math solutions)
- code: 1.9M samples (programming challenges)
- chat: 746K samples (conversational tuning)
- tool_calling: 310K samples (function calling)

Usage:
    # Download all splits to separate folders (recommended)
    python download_nemotron_v1.py --sample-percent 30 --tokenizer Qwen/Qwen3-8B --include-reasoning
    
    # Download specific splits
    python download_nemotron_v1.py --splits stem,math --sample-percent 30 --include-reasoning
    
    # Combined mode (legacy - all splits in one file)
    python download_nemotron_v1.py --sample-percent 30 --combined

Output structure (split mode - default):
    nemotron_v1/
    ‚îú‚îÄ‚îÄ stem/
    ‚îÇ   ‚îú‚îÄ‚îÄ stem_30pct_cot_chat_train.jsonl
    ‚îÇ   ‚îú‚îÄ‚îÄ stem_30pct_cot_chat_validation.jsonl
    ‚îÇ   ‚îî‚îÄ‚îÄ stem_30pct_cot_chat_test.jsonl
    ‚îú‚îÄ‚îÄ math/
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ metadata.json

Output structure (combined mode):
    nemotron_v1/
    ‚îú‚îÄ‚îÄ nemotron_all_30pct_cot_chat_train.jsonl
    ‚îî‚îÄ‚îÄ ...
"""

import argparse
import json
import os
import random
from datasets import load_dataset
from tqdm import tqdm

DEFAULT_OUTPUT_DIR = "/lustre/fsw/coreai_dlalgo_modelopt/weimingc/datasets/nemotron_v1"
DATABLEND_DIR = "/lustre/fsw/coreai_dlalgo_modelopt/weimingc/datasets"

# Available splits and their sizes
AVAILABLE_SPLITS = {
    "stem": 20662167,      # Best for MMLU - science, reasoning, humanities
    "math": 2044407,       # Math reasoning
    "code": 1896395,       # Code challenges
    "chat": 746622,        # Conversational
    "tool_calling": 310051 # Function calling
}

# Train/valid/test split ratios
TRAIN_RATIO = 0.95
VALID_RATIO = 0.025
TEST_RATIO = 0.025
RANDOM_SEED = 42

# Global tokenizer for chat template (initialized if --tokenizer is provided)
_TOKENIZER = None


def init_tokenizer(tokenizer_name: str):
    """Initialize tokenizer for chat template formatting."""
    global _TOKENIZER
    if tokenizer_name:
        from transformers import AutoTokenizer
        print(f"üìù Loading tokenizer for chat template: {tokenizer_name}")
        _TOKENIZER = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
        
        # Show example
        example = [{"role": "user", "content": "Hi"}, {"role": "assistant", "content": "Hello!"}]
        formatted = _TOKENIZER.apply_chat_template(example, tokenize=False)
        print(f"   Example format:\n   {formatted[:200]}...")


def format_messages_to_text(messages: list, reasoning: str = None) -> str:
    """Convert messages format to text for QAD training.
    
    If a tokenizer is initialized, uses its chat template.
    Otherwise, uses simple role-based formatting.
    """
    global _TOKENIZER
    
    # Optionally prepend reasoning/chain-of-thought
    if reasoning and reasoning.strip():
        # Insert thinking block before last assistant message
        messages_with_cot = []
        for i, msg in enumerate(messages):
            if msg.get("role") == "assistant" and i == len(messages) - 1:
                thinking_content = f"<think>\n{reasoning}\n</think>\n{msg.get('content', '')}"
                messages_with_cot.append({"role": "assistant", "content": thinking_content})
            else:
                messages_with_cot.append(msg)
        messages = messages_with_cot
    
    # Use chat template if tokenizer is available
    if _TOKENIZER is not None:
        try:
            return _TOKENIZER.apply_chat_template(messages, tokenize=False)
        except Exception as e:
            print(f"Warning: Chat template failed, using simple format: {e}")
    
    # Fallback: simple role-based format
    text_parts = []
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        
        if role == "system":
            text_parts.append(f"System: {content}")
        elif role == "user":
            text_parts.append(f"User: {content}")
        elif role == "assistant":
            text_parts.append(f"Assistant: {content}")
    
    return "\n\n".join(text_parts)


def download_split(split_name: str, max_samples: int, output_dir: str,
                   suffix: str, include_reasoning: bool = False) -> dict:
    """Download a single split and save to its own folder.
    
    Returns dict with sample counts for each partition.
    """
    print(f"\nüì• Downloading split: {split_name} (target: {max_samples:,} samples)")
    
    # Create split-specific directory
    split_dir = os.path.join(output_dir, split_name)
    os.makedirs(split_dir, exist_ok=True)
    
    try:
        # Load the specific split with streaming
        dataset = load_dataset(
            "nvidia/Nemotron-Post-Training-Dataset-v1",
            split=split_name,
            streaming=True
        )
        
        all_examples = []
        count = 0
        
        for example in tqdm(dataset, desc=f"Processing {split_name}", total=max_samples):
            if count >= max_samples:
                break
            
            messages = example.get("messages", [])
            reasoning = example.get("reasoning", "") if include_reasoning else ""
            
            # Convert to text format
            text = format_messages_to_text(messages, reasoning)
            
            if text.strip():
                all_examples.append({
                    "text": text,
                    "category": example.get("category", split_name),
                })
                count += 1
        
        print(f"‚úì Collected {count:,} examples from {split_name}")
        
        if not all_examples:
            print(f"Warning: No examples collected for {split_name}")
            return {"train": 0, "validation": 0, "test": 0}
        
        # Shuffle and split into train/valid/test
        random.seed(RANDOM_SEED)
        random.shuffle(all_examples)
        
        total_size = len(all_examples)
        train_end = int(total_size * TRAIN_RATIO)
        valid_end = train_end + int(total_size * VALID_RATIO)
        
        partitions = {
            'train': all_examples[:train_end],
            'validation': all_examples[train_end:valid_end],
            'test': all_examples[valid_end:]
        }
        
        # Save each partition
        counts = {}
        for part_name, part_data in partitions.items():
            output_file = os.path.join(split_dir, f"{split_name}_{suffix}_{part_name}.jsonl")
            
            with open(output_file, 'w', encoding='utf-8') as f:
                for example in part_data:
                    json_line = json.dumps({"text": example["text"]}, ensure_ascii=False)
                    f.write(json_line + '\n')
            
            counts[part_name] = len(part_data)
            print(f"   ‚úì {part_name}: {len(part_data):,} samples ‚Üí {output_file}")
        
        return counts
        
    except Exception as e:
        print(f"Error loading {split_name}: {e}")
        return {"train": 0, "validation": 0, "test": 0}


def create_datablend_configs(output_dir: str, splits_downloaded: list, suffix: str, 
                             sample_counts: dict):
    """Create datablend JSON configs for each split and combined."""
    preprocessed_dir = output_dir.replace("nemotron_v1", "nemotron_v1_preprocessed")
    
    # Create individual datablend for each split
    for split_name in splits_downloaded:
        blend_file = os.path.join(DATABLEND_DIR, f"datablend_nemotron_v1_{split_name}_{suffix}.json")
        blend_config = {
            "train": [1.0, f"{preprocessed_dir}/{split_name}/{split_name}_{suffix}_train_text_document"],
            "valid": [1.0, f"{preprocessed_dir}/{split_name}/{split_name}_{suffix}_validation_text_document"],
            "test": [1.0, f"{preprocessed_dir}/{split_name}/{split_name}_{suffix}_test_text_document"]
        }
        
        with open(blend_file, 'w') as f:
            json.dump(blend_config, f, indent=2)
        print(f"üìù Created: {blend_file}")
    
    # Create combined datablend (all English splits)
    if len(splits_downloaded) > 1:
        english_splits = [s for s in splits_downloaded if s in ["stem", "math", "code", "chat"]]
        if english_splits:
            # Calculate weights based on sample counts
            total_samples = sum(sample_counts.get(s, {}).get("train", 0) for s in english_splits)
            
            blend_file = os.path.join(DATABLEND_DIR, f"datablend_nemotron_v1_all_en_{suffix}.json")
            
            train_entries = []
            valid_entries = []
            test_entries = []
            
            for split_name in english_splits:
                split_count = sample_counts.get(split_name, {}).get("train", 0)
                weight = split_count / total_samples if total_samples > 0 else 1.0 / len(english_splits)
                
                train_entries.extend([
                    weight,
                    f"{preprocessed_dir}/{split_name}/{split_name}_{suffix}_train_text_document"
                ])
                valid_entries.extend([
                    weight,
                    f"{preprocessed_dir}/{split_name}/{split_name}_{suffix}_validation_text_document"
                ])
                test_entries.extend([
                    weight,
                    f"{preprocessed_dir}/{split_name}/{split_name}_{suffix}_test_text_document"
                ])
            
            blend_config = {
                "train": train_entries,
                "valid": valid_entries,
                "test": test_entries
            }
            
            with open(blend_file, 'w') as f:
                json.dump(blend_config, f, indent=2)
            print(f"üìù Created combined: {blend_file}")


def main():
    parser = argparse.ArgumentParser(description="Download Nemotron-v1 for QAD")
    parser.add_argument("--output-dir", type=str, default=DEFAULT_OUTPUT_DIR,
                        help="Output directory for JSONL files")
    parser.add_argument("--splits", type=str, default="stem,math,code,chat",
                        help="Comma-separated list of splits to download (stem,math,code,chat,tool_calling)")
    parser.add_argument("--sample-percent", type=float, default=30.0,
                        help="Percentage of each split to use (1-100). E.g., 10 = 10%% of each split")
    parser.add_argument("--max-samples", type=int, default=None,
                        help="Maximum samples per split (absolute cap, used if --sample-percent not set)")
    parser.add_argument("--include-reasoning", action="store_true",
                        help="Include chain-of-thought reasoning in output")
    parser.add_argument("--tokenizer", type=str, default=None,
                        help="HuggingFace tokenizer to use for chat template (e.g., Qwen/Qwen3-8B)")
    parser.add_argument("--combined", action="store_true",
                        help="Legacy mode: combine all splits into single files instead of separate folders")
    args = parser.parse_args()
    
    # Default to 30% if neither option is specified
    if args.sample_percent is None and args.max_samples is None:
        args.sample_percent = 30.0
    
    # Initialize tokenizer if specified
    if args.tokenizer:
        init_tokenizer(args.tokenizer)
    
    output_dir = args.output_dir
    os.makedirs(output_dir, exist_ok=True)
    
    splits_to_download = [s.strip() for s in args.splits.split(",")]
    
    # Build suffix string
    if args.sample_percent is not None:
        pct_str = f"{int(args.sample_percent)}pct"
    else:
        pct_str = ""
    
    cot_str = "_cot" if args.include_reasoning else ""
    chat_str = "_chat" if args.tokenizer else ""
    suffix = f"{pct_str}{cot_str}{chat_str}"
    
    print("=" * 70)
    print("Downloading NVIDIA Nemotron-Post-Training-Dataset-v1")
    print("=" * 70)
    print(f"Mode: {'Combined (legacy)' if args.combined else 'Split (fine-grained)'}")
    print(f"Splits: {splits_to_download}")
    print(f"Sample percent: {args.sample_percent}%")
    print(f"Include reasoning: {args.include_reasoning}")
    print(f"Chat template: {args.tokenizer or 'None (simple format)'}")
    print(f"Suffix: {suffix}")
    print(f"Output directory: {output_dir}")
    print("=" * 70)
    
    # Calculate samples per split (with 500K cap per split)
    MAX_SAMPLES_PER_SPLIT = 500000  # Cap at 500K per split for manageable dataset size
    samples_per_split = {}
    for split_name in splits_to_download:
        if split_name not in AVAILABLE_SPLITS:
            continue
        available = AVAILABLE_SPLITS[split_name]
        if args.sample_percent is not None:
            calculated = int(available * args.sample_percent / 100)
            samples_per_split[split_name] = min(calculated, MAX_SAMPLES_PER_SPLIT)
        else:
            samples_per_split[split_name] = min(available, args.max_samples, MAX_SAMPLES_PER_SPLIT)
    
    print(f"\nExpected samples per split (capped at {MAX_SAMPLES_PER_SPLIT:,}):")
    total_expected = 0
    for split_name, count in samples_per_split.items():
        available = AVAILABLE_SPLITS[split_name]
        pct = count / available * 100
        capped = " (CAPPED)" if count == MAX_SAMPLES_PER_SPLIT else ""
        print(f"  {split_name}: {count:,} / {available:,} ({pct:.1f}%){capped}")
        total_expected += count
    print(f"  Total expected: {total_expected:,}")
    
    if args.combined:
        # Legacy combined mode
        download_combined_mode(args, splits_to_download, samples_per_split, suffix)
    else:
        # New split mode (default)
        sample_counts = {}
        
        for split_name in splits_to_download:
            if split_name not in AVAILABLE_SPLITS:
                print(f"Warning: Unknown split '{split_name}', skipping...")
                continue
            
            max_samples = samples_per_split[split_name]
            counts = download_split(
                split_name=split_name,
                max_samples=max_samples,
                output_dir=output_dir,
                suffix=suffix,
                include_reasoning=args.include_reasoning
            )
            sample_counts[split_name] = counts
        
        # Save metadata
        metadata = {
            "sample_percent": args.sample_percent,
            "include_reasoning": args.include_reasoning,
            "tokenizer": args.tokenizer,
            "suffix": suffix,
            "splits": {}
        }
        for split_name, counts in sample_counts.items():
            metadata["splits"][split_name] = {
                "train": counts.get("train", 0),
                "validation": counts.get("validation", 0),
                "test": counts.get("test", 0),
                "total": sum(counts.values())
            }
        
        metadata_file = os.path.join(output_dir, f"metadata_{suffix}.json")
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"\nüìù Saved metadata: {metadata_file}")
        
        # Create datablend configs
        print("\n" + "=" * 70)
        print("Creating datablend configs...")
        create_datablend_configs(output_dir, list(sample_counts.keys()), suffix, sample_counts)
        
        # Print summary
        print("\n" + "=" * 70)
        print("‚úì Nemotron-v1 download complete!")
        print("=" * 70)
        print(f"\nOutput structure:")
        print(f"  {output_dir}/")
        for split_name in sample_counts.keys():
            print(f"  ‚îú‚îÄ‚îÄ {split_name}/")
            print(f"  ‚îÇ   ‚îú‚îÄ‚îÄ {split_name}_{suffix}_train.jsonl")
            print(f"  ‚îÇ   ‚îú‚îÄ‚îÄ {split_name}_{suffix}_validation.jsonl")
            print(f"  ‚îÇ   ‚îî‚îÄ‚îÄ {split_name}_{suffix}_test.jsonl")
        print(f"  ‚îî‚îÄ‚îÄ metadata_{suffix}.json")
        
        print(f"\nSample counts:")
        total_train = 0
        for split_name, counts in sample_counts.items():
            train_count = counts.get("train", 0)
            total_train += train_count
            print(f"  {split_name}: {train_count:,} train samples")
        print(f"  Total: {total_train:,} train samples")
        
        print(f"\nNext steps:")
        print(f"1. Preprocess each split:")
        for split_name in sample_counts.keys():
            print(f"   bash process_nemotron_v1_qwen3-8B.sh {split_name} {suffix}")
        print(f"\n2. Use individual splits:")
        print(f"   DATASET_NAME=nemotron_v1_stem_{suffix} bash qwen_qad.sh ...")
        print(f"\n3. Or use combined datablend:")
        print(f"   BLEND_PATH=datablend_nemotron_v1_all_en_{suffix}.json bash qwen_qad.sh ...")
        print("=" * 70)


def download_combined_mode(args, splits_to_download, samples_per_split, suffix):
    """Legacy combined mode - all splits in single files."""
    output_dir = args.output_dir
    
    all_examples = []
    
    for split_name in splits_to_download:
        if split_name not in AVAILABLE_SPLITS:
            print(f"Warning: Unknown split '{split_name}', skipping...")
            continue
        
        max_for_split = samples_per_split[split_name]
        print(f"\nüì• Loading split: {split_name} (target: {max_for_split:,} samples)")
        
        try:
            dataset = load_dataset(
                "nvidia/Nemotron-Post-Training-Dataset-v1",
                split=split_name,
                streaming=True
            )
            
            count = 0
            for example in tqdm(dataset, desc=f"Processing {split_name}", total=max_for_split):
                if count >= max_for_split:
                    break
                
                messages = example.get("messages", [])
                reasoning = example.get("reasoning", "") if args.include_reasoning else ""
                
                text = format_messages_to_text(messages, reasoning)
                
                if text.strip():
                    all_examples.append({
                        "text": text,
                        "category": example.get("category", split_name),
                        "source": "nemotron_v1"
                    })
                    count += 1
            
            print(f"‚úì Collected {count:,} examples from {split_name}")
            
        except Exception as e:
            print(f"Error loading {split_name}: {e}")
            continue
    
    if not all_examples:
        print("Error: No examples collected!")
        return
    
    print(f"\nüìä Total examples collected: {len(all_examples):,}")
    
    # Shuffle and split
    random.seed(RANDOM_SEED)
    random.shuffle(all_examples)
    
    total_size = len(all_examples)
    train_end = int(total_size * TRAIN_RATIO)
    valid_end = train_end + int(total_size * VALID_RATIO)
    
    splits = {
        'train': all_examples[:train_end],
        'validation': all_examples[train_end:valid_end],
        'test': all_examples[valid_end:]
    }
    
    # Generate output filename
    if set(splits_to_download) >= {"stem", "math", "code", "chat"}:
        split_suffix = "all"
    else:
        split_suffix = "_".join(splits_to_download)
    
    full_suffix = f"_{suffix}" if suffix else ""
    
    # Save each split as JSONL
    for split_name, split_data in splits.items():
        output_file = os.path.join(output_dir, f"nemotron_{split_suffix}{full_suffix}_{split_name}.jsonl")
        print(f"\nWriting {output_file}...")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for example in tqdm(split_data, desc=split_name):
                json_line = json.dumps({"text": example["text"]}, ensure_ascii=False)
                f.write(json_line + '\n')
        
        print(f"‚úì Saved {split_name}")
    
    # Create datablend config
    blend_file = os.path.join(DATABLEND_DIR, f"datablend_nemotron_{split_suffix}{full_suffix}.json")
    
    preprocessed_dir = output_dir.replace("nemotron_v1", "nemotron_v1_preprocessed")
    blend_config = {
        "train": [1.0, f"{preprocessed_dir}/nemotron_{split_suffix}{full_suffix}_train_text_document"],
        "valid": [1.0, f"{preprocessed_dir}/nemotron_{split_suffix}{full_suffix}_validation_text_document"],
        "test": [1.0, f"{preprocessed_dir}/nemotron_{split_suffix}{full_suffix}_test_text_document"]
    }
    
    with open(blend_file, 'w') as f:
        json.dump(blend_config, f, indent=2)
    print(f"\nüìù Saved datablend config: {blend_file}")
    
    print("\n" + "=" * 70)
    print("‚úì Nemotron-v1 download complete (combined mode)!")
    print(f"Output directory: {output_dir}")
    print(f"\nDataset summary:")
    print(f"  Total samples: {len(all_examples):,}")
    print(f"  Train: {len(splits['train']):,}")
    print(f"  Validation: {len(splits['validation']):,}")
    print(f"  Test: {len(splits['test']):,}")
    print("\nNext steps:")
    print(f"1. Preprocess: bash process_nemotron_qwen3-8B.sh {split_suffix}{full_suffix}")
    print(f"2. Run QAD: DATASET_NAME=nemotron_{split_suffix}{full_suffix} bash qwen_qad.sh ...")
    print("=" * 70)


if __name__ == "__main__":
    main()
