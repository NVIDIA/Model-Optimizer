{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "efe23925",
      "metadata": {},
      "source": [
        "# Training an EAGLE3 Draft Head for Cosmos-Reason2\n",
        "\n",
        "This notebook walks through the full workflow for training an EAGLE3 speculative-decoding draft head on top of [nvidia/Cosmos-Reason2-8B](https://huggingface.co/nvidia/Cosmos-Reason2-8B).\n",
        "\n",
        "**Workflow overview**\n",
        "\n",
        "| Step | Description |\n",
        "| :---: | :--- |\n",
        "| 1 | Install dependencies |\n",
        "| 2 | Authenticate with Hugging Face |\n",
        "| 3 | Prepare training data from the Nemotron dataset |\n",
        "| 4 | Inspect the bundled EAGLE3 config for Cosmos-Reason2 |\n",
        "| 5 | Calibrate the draft vocabulary |\n",
        "| 6 | Launch training |\n",
        "| 7 | Export checkpoint for deployment |\n",
        "\n",
        "> **Hardware requirement** – Cosmos-Reason2-8B requires at least one 80 GB GPU (e.g. H100/A100).\n",
        "> Multi-GPU training is supported automatically via FSDP2 when more than one GPU is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 – Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ModelOpt with Hugging Face support and the example-specific requirements.\n",
        "# Run once; restart the kernel afterwards if running inside a fresh environment.\n",
        "!pip install -U nvidia-modelopt[hf]\n",
        "!pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 – Authenticate with Hugging Face\n",
        "\n",
        "Both `nvidia/Cosmos-Reason2-8B` and `nvidia/Nemotron-Post-Training-Dataset-v2` require you to\n",
        "accept their respective licence agreements on the Hub before downloading.  \n",
        "Log in once with your HF token (needs `read` scope):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your token or set the HF_TOKEN environment variable before running.\n",
        "# login(token=\"hf_...\")\n",
        "login()  # interactive prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdd4d470",
      "metadata": {},
      "source": [
        "## Step 3 – Prepare Training Data\n",
        "\n",
        "We use a curated subset of [nvidia/Nemotron-Post-Training-Dataset-v2](https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v2)\n",
        "(chat split) for training.  The `guides/nemotron_mapping.csv` file selects the specific rows to use.\n",
        "\n",
        "The script streams only the required parquet shards and writes a conversation file in the\n",
        "standard `jsonl` format expected by `launch_train.sh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Run from the speculative_decoding root so that relative paths resolve correctly.\n",
        "os.chdir(\"..\")\n",
        "print(\"Working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32259e23",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python prepare_input_conversations/add_nemotron_chat.py \\\n",
        "    --mapping-file guides/nemotron_mapping.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05b97d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the output: the file should contain exactly 89 511 conversations.\n",
        "data_path = \"input_conversations/nemotron-chat.jsonl\"\n",
        "\n",
        "with open(data_path) as f:\n",
        "    line_count = sum(1 for _ in f)\n",
        "\n",
        "assert line_count == 89511, f\"Expected 89511 lines, got {line_count}\"\n",
        "print(f\"✓ {line_count} conversations written to {data_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09717fcc",
      "metadata": {},
      "source": [
        "## Step 4 – EAGLE3 Config for Cosmos-Reason2\n",
        "\n",
        "The `CR2_eagle_config.json` file is bundled alongside this notebook in the `guides/` directory.\n",
        "It overrides the ModelOpt defaults with settings tuned for Cosmos-Reason2-8B (YaRN RoPE,\n",
        "reduced draft vocabulary, FlexAttention).  No action is needed here — the cell below simply\n",
        "prints the config for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388f6897",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "config_path = Path(\"guides/CR2_eagle_config.json\")\n",
        "config = json.loads(config_path.read_text())\n",
        "\n",
        "print(f\"Config: {config_path}\")\n",
        "print(json.dumps(config, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34fe137f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path passed to launch_train.sh (relative to the speculative_decoding root).\n",
        "EAGLE_CONFIG = \"guides/CR2_eagle_config.json\"\n",
        "print(\"Eagle config path:\", EAGLE_CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "336c43b9",
      "metadata": {},
      "source": [
        "## Step 5 – Calibrate the Draft Vocabulary *(optional)*\n",
        "\n",
        "`CR2_eagle_config.json` sets `\"draft_vocab_size\": 32000`.  Using a compressed vocabulary\n",
        "speeds up training and inference, but requires a one-time calibration step that produces a\n",
        "token-mapping file (`d2t.pt`).  Skip this step if you prefer to train with the full vocabulary\n",
        "(remove `--draft_vocab_cache` from the training command in Step 6 in that case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0380f773",
      "metadata": {},
      "outputs": [],
      "source": [
        "DRAFT_VOCAB_CACHE_DIR = \"draft_vocab_cache\"\n",
        "\n",
        "!python scripts/calibrate_draft_vocab.py \\\n",
        "    --model nvidia/Cosmos-Reason2-8B \\\n",
        "    --data input_conversations/nemotron-chat.jsonl \\\n",
        "    --draft_vocab_size 32000 \\\n",
        "    --save_dir {DRAFT_VOCAB_CACHE_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e0f8c4",
      "metadata": {},
      "source": [
        "## Step 6 – Train the EAGLE3 Draft Head\n",
        "\n",
        "Training is launched via `launch_train.sh`, which internally calls `accelerate launch main.py`\n",
        "and sets up FSDP2 automatically when multiple GPUs are available.\n",
        "\n",
        "Key arguments used for Cosmos-Reason2:\n",
        "\n",
        "| Argument | Value | Notes |\n",
        "| :--- | :--- | :--- |\n",
        "| `--model` | `nvidia/Cosmos-Reason2-8B` | Target VLM |\n",
        "| `--data` | `input_conversations/nemotron-chat.jsonl` | Training conversations |\n",
        "| `--eagle_config` | `guides/CR2_eagle_config.json` | Draft-head architecture |\n",
        "| `--draft_vocab_cache` | `draft_vocab_cache/d2t.pt` | Token-mapping file from Step 5 *(optional)* |\n",
        "| `--vlm_processor` | `nvidia/Cosmos-Reason2-8B` | VLM image processor |\n",
        "| `--vlm_img_dir` | `data/` | Directory containing referenced images |\n",
        "| `--training_seq_len` | `8192` | Max token length per sample |\n",
        "| `--lr` | `1.5e-4` | Learning rate |\n",
        "| `--num_epochs` | `20` | Training epochs |\n",
        "| `--train_bs` | `1` | Per-device batch size |\n",
        "| `--save_steps` | `1000` | Checkpoint frequency |\n",
        "| `--ar_validate_steps` | `1000000` | Effectively disables in-training AR validation |\n",
        "\n",
        "> **Tip** – Set `--ar_validate_steps` to a smaller value (e.g. `500`) to periodically measure\n",
        "> acceptance rate on MT-Bench during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63880f67",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"ckpts/cosmos-reason2-8b-eagle3\"\n",
        "EAGLE_CONFIG = \"guides/CR2_eagle_config.json\"\n",
        "DRAFT_VOCAB_CACHE = \"draft_vocab_cache/Cosmos-Reason2-8B/d2t.pt\"  # set to None to skip vocab compression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc69d54c",
      "metadata": {},
      "outputs": [],
      "source": [
        "draft_vocab_arg = f\"--draft_vocab_cache {DRAFT_VOCAB_CACHE}\" if DRAFT_VOCAB_CACHE else \"\"\n",
        "\n",
        "# Outputs are streamed live.  Training 20 epochs on 89k samples with a single H100\n",
        "# takes approximately 12–24 hours depending on sequence length distribution.\n",
        "!export WANDB_MODE=disabled && OUTPUT_DIR={OUTPUT_DIR} \\\n",
        "  ./launch_train.sh \\\n",
        "  --model nvidia/Cosmos-Reason2-8B \\\n",
        "  --output_dir {OUTPUT_DIR} \\\n",
        "  --data input_conversations/nemotron-chat.jsonl \\\n",
        "  --lr 1.5e-4 \\\n",
        "  --num_epochs 20 \\\n",
        "  --train_bs 1 \\\n",
        "  --eagle_config {EAGLE_CONFIG} \\\n",
        "  {draft_vocab_arg} \\\n",
        "  --training_seq_len 8192 \\\n",
        "  --save_steps 1000 \\\n",
        "  --ar_validate_steps 1000000 \\\n",
        "  --vlm_processor nvidia/Cosmos-Reason2-8B \\\n",
        "  --vlm_img_dir data/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "413c4275",
      "metadata": {},
      "source": [
        "## Step 7 – Export Checkpoint for Deployment\n",
        "\n",
        "After training completes, convert the ModelOpt checkpoint to the Hugging Face–compatible\n",
        "format expected by vLLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"ckpts/cosmos-reason2-8b-eagle3\"\n",
        "EXPORT_PATH = \"export/cosmos-reason2-8b-eagle3\"\n",
        "\n",
        "!python scripts/export_hf_checkpoint.py \\\n",
        "    --model_path {OUTPUT_DIR} \\\n",
        "    --export_path {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82b82aad",
      "metadata": {},
      "source": [
        "## Deployment\n",
        "\n",
        "The exported checkpoint can be served directly with **vLLM**:\n",
        "\n",
        "```bash\n",
        "vllm serve nvidia/Cosmos-Reason2-8B \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --port 8000 \\\n",
        "    --speculative-model export/cosmos-reason2-8b-eagle3 \\\n",
        "    --num-speculative-tokens 3 \\\n",
        "    --dtype bfloat16\n",
        "```\n",
        "\n",
        "Refer to the [vLLM speculative decoding docs](https://docs.vllm.ai/en/latest/features/spec_decode/) for the full list of options."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
